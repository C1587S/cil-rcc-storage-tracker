#!/usr/bin/env python3
"""
Parquet Snapshot Validation Script

Validates parquet files generated by the scanner to ensure:
- Files exist and are readable
- Required schema columns are present
- Data integrity is maintained
- Row counts are reasonable

Usage:
    python validate_parquet.py <snapshot_directory>
    python validate_parquet.py /path/to/data/snapshots/2024-01-15
"""

import sys
import os
from pathlib import Path
from typing import Dict, List, Optional
import pyarrow.parquet as pq


# Required columns in the parquet schema
REQUIRED_COLUMNS = {
    'path',
    'size',
    'modified_time',
    'file_type',
    'parent_path',
    'depth',
    'top_level_dir'
}

# Optional columns (nice to have but not required)
OPTIONAL_COLUMNS = {
    'accessed_time',
    'created_time',
    'inode',
    'permissions'
}


class ValidationResult:
    """Container for validation results"""

    def __init__(self):
        self.success = True
        self.errors: List[str] = []
        self.warnings: List[str] = []
        self.file_count = 0
        self.total_rows = 0
        self.files_validated: List[str] = []

    def add_error(self, message: str):
        self.errors.append(message)
        self.success = False

    def add_warning(self, message: str):
        self.warnings.append(message)

    def print_summary(self):
        """Print validation summary"""
        print("\n" + "=" * 60)
        print("VALIDATION SUMMARY")
        print("=" * 60)

        if self.success:
            print("✓ Status: PASSED")
        else:
            print("✗ Status: FAILED")

        print(f"\nFiles validated: {self.file_count}")
        print(f"Total rows: {self.total_rows:,}")

        if self.errors:
            print(f"\nErrors: {len(self.errors)}")
            for error in self.errors:
                print(f"  ✗ {error}")

        if self.warnings:
            print(f"\nWarnings: {len(self.warnings)}")
            for warning in self.warnings:
                print(f"  ⚠ {warning}")

        if self.success and not self.warnings:
            print("\n✓ All validations passed successfully")

        print("=" * 60)


def validate_snapshot_directory(snapshot_dir: str) -> ValidationResult:
    """
    Validate all parquet files in a snapshot directory

    Args:
        snapshot_dir: Path to snapshot directory containing parquet files

    Returns:
        ValidationResult object with validation details
    """
    result = ValidationResult()
    path = Path(snapshot_dir)

    # Check directory exists
    if not path.exists():
        result.add_error(f"Directory does not exist: {snapshot_dir}")
        return result

    if not path.is_dir():
        result.add_error(f"Path is not a directory: {snapshot_dir}")
        return result

    # Find all parquet files
    parquet_files = list(path.glob("*.parquet"))

    if not parquet_files:
        result.add_error("No parquet files found in directory")
        return result

    print(f"Found {len(parquet_files)} parquet file(s) to validate\n")

    # Validate each file
    for pq_file in parquet_files:
        validate_parquet_file(pq_file, result)

    return result


def validate_parquet_file(file_path: Path, result: ValidationResult):
    """
    Validate a single parquet file

    Args:
        file_path: Path to parquet file
        result: ValidationResult to update
    """
    file_name = file_path.name
    print(f"Validating: {file_name}")

    try:
        # Read parquet file
        table = pq.read_table(file_path)
        rows = len(table)

        # Update counts
        result.file_count += 1
        result.total_rows += rows
        result.files_validated.append(file_name)

        print(f"  Rows: {rows:,}")

        # Validate schema
        actual_columns = set(table.column_names)
        print(f"  Columns: {len(actual_columns)}")

        # Check required columns
        missing_columns = REQUIRED_COLUMNS - actual_columns
        if missing_columns:
            result.add_error(
                f"{file_name}: Missing required columns: {missing_columns}"
            )
            return

        # Check for optional columns
        missing_optional = OPTIONAL_COLUMNS - actual_columns
        if missing_optional:
            result.add_warning(
                f"{file_name}: Missing optional columns: {missing_optional}"
            )

        # Validate data types
        validate_column_types(table, file_name, result)

        # Validate data quality
        validate_data_quality(table, file_name, result)

        print(f"  ✓ Schema validation passed")

        # Check for reasonable row count
        if rows == 0:
            result.add_warning(f"{file_name}: File contains 0 rows")
        elif rows < 10:
            result.add_warning(
                f"{file_name}: File contains very few rows ({rows})"
            )

    except Exception as e:
        result.add_error(f"{file_name}: Failed to read parquet file: {str(e)}")


def validate_column_types(table, file_name: str, result: ValidationResult):
    """Validate column data types"""

    # Path should be string
    if 'path' in table.column_names:
        path_col = table.column('path')
        if not str(path_col.type).startswith('string'):
            result.add_warning(
                f"{file_name}: 'path' column should be string type"
            )

    # Size should be numeric
    if 'size' in table.column_names:
        size_col = table.column('size')
        if not str(size_col.type).startswith(('int', 'uint')):
            result.add_warning(
                f"{file_name}: 'size' column should be integer type"
            )

    # Depth should be numeric
    if 'depth' in table.column_names:
        depth_col = table.column('depth')
        if not str(depth_col.type).startswith(('int', 'uint')):
            result.add_warning(
                f"{file_name}: 'depth' column should be integer type"
            )


def validate_data_quality(table, file_name: str, result: ValidationResult):
    """Validate data quality and consistency"""

    # Convert to pandas for easier analysis
    try:
        df = table.to_pandas()

        # Check for null paths
        if 'path' in df.columns:
            null_paths = df['path'].isna().sum()
            if null_paths > 0:
                result.add_error(
                    f"{file_name}: Found {null_paths} null path values"
                )

        # Check for negative sizes
        if 'size' in df.columns:
            negative_sizes = (df['size'] < 0).sum()
            if negative_sizes > 0:
                result.add_error(
                    f"{file_name}: Found {negative_sizes} negative size values"
                )

        # Check for negative depths
        if 'depth' in df.columns:
            negative_depths = (df['depth'] < 0).sum()
            if negative_depths > 0:
                result.add_error(
                    f"{file_name}: Found {negative_depths} negative depth values"
                )

        # Check for empty file types
        if 'file_type' in df.columns:
            empty_types = df['file_type'].isna().sum()
            if empty_types > 0:
                result.add_warning(
                    f"{file_name}: Found {empty_types} empty file_type values"
                )

    except Exception as e:
        result.add_warning(
            f"{file_name}: Could not validate data quality: {str(e)}"
        )


def main():
    """Main entry point"""
    if len(sys.argv) != 2:
        print("Usage: python validate_parquet.py <snapshot_directory>")
        print("\nExample:")
        print("  python validate_parquet.py data/snapshots/2024-01-15")
        sys.exit(1)

    snapshot_dir = sys.argv[1]

    print("Parquet Snapshot Validation")
    print(f"Directory: {snapshot_dir}\n")

    # Run validation
    result = validate_snapshot_directory(snapshot_dir)

    # Print summary
    result.print_summary()

    # Exit with appropriate code
    sys.exit(0 if result.success else 1)


if __name__ == "__main__":
    main()

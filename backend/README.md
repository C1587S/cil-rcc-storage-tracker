# Storage Analytics Backend (Phase 2)

FastAPI backend for querying and analyzing storage snapshots generated by the Rust scanner.

## Features

- **Fast OLAP Queries**: DuckDB for efficient analytical queries over Parquet files
- **Advanced Search**: Regex and glob pattern matching across file metadata
- **Folder Analytics**: Hierarchical breakdown, time-series analysis, and distribution
- **Smart Caching**: Redis-based caching for improved performance
- **RESTful API**: Well-documented FastAPI endpoints with OpenAPI/Swagger docs
- **Type Safety**: Full Pydantic models with validation

## Tech Stack

- **FastAPI**: Modern async web framework
- **DuckDB**: Embedded OLAP database
- **Polars**: High-performance DataFrame library
- **Redis**: Caching layer
- **PyArrow**: Apache Arrow integration
- **Uvicorn**: ASGI server

## Quick Start

### Option 1: Using Docker (Recommended)

```bash
# Start all services
docker-compose up -d

# View logs
docker-compose logs -f backend

# Stop services
docker-compose down
```

The API will be available at [http://localhost:8000](http://localhost:8000)

### Option 2: Using Virtual Environment

```bash
# Run setup script
./scripts/setup_dev.sh

# Activate virtual environment
source venv/bin/activate

# Start development server
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### Option 3: Using Conda

```bash
# Run setup script
./scripts/setup_conda.sh

# Activate environment
conda activate storage-analytics

# Start development server
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

## Project Structure

```
backend/
├── app/
│   ├── main.py                 # FastAPI application entry
│   ├── config.py               # Configuration management
│   ├── database/               # Database layer
│   │   ├── duckdb_client.py   # DuckDB client
│   │   ├── schema.py          # Schema initialization
│   │   └── queries.py         # Query builders
│   ├── models/                # Pydantic models
│   │   ├── file_entry.py     # File entry models
│   │   ├── snapshot.py       # Snapshot models
│   │   └── response.py       # API response models
│   ├── routers/               # API endpoints
│   │   ├── health.py         # Health checks
│   │   ├── snapshots.py      # Snapshot management
│   │   ├── search.py         # File search
│   │   ├── folders.py        # Folder analysis
│   │   └── analytics.py      # Analytics endpoints
│   ├── services/              # Business logic
│   │   ├── cache_service.py  # Redis caching
│   │   ├── search_service.py # Search operations
│   │   ├── folder_service.py # Folder operations
│   │   └── snapshot_service.py # Snapshot operations
│   └── utils/                 # Utilities
│       ├── formatters.py     # Data formatting
│       └── validators.py     # Input validation
├── tests/                     # Test suite
├── scripts/                   # Setup and utility scripts
├── requirements.txt          # Production dependencies
├── requirements-dev.txt      # Development dependencies
├── Dockerfile               # Docker image
├── docker-compose.yml       # Docker Compose config
└── README.md               # This file
```

## API Documentation

Once the server is running, visit:

- **Interactive API docs**: [http://localhost:8000/docs](http://localhost:8000/docs)
- **Alternative docs**: [http://localhost:8000/redoc](http://localhost:8000/redoc)
- **OpenAPI JSON**: [http://localhost:8000/openapi.json](http://localhost:8000/openapi.json)

## API Endpoints

### Health & Status

- `GET /health` - Health check
- `GET /health/db` - Database health check

### Snapshots

- `GET /api/snapshots` - List all snapshots
- `GET /api/snapshots/latest` - Get latest snapshot
- `GET /api/snapshots/{date}` - Get specific snapshot details
- `GET /api/snapshots/compare?from=date&to=date` - Compare two snapshots

### Search

- `GET /api/search?q=pattern` - Search files by pattern
- `GET /api/search/history?path=/file/path` - Get file history across snapshots
- `POST /api/search/advanced` - Advanced search with multiple filters

### Folders

- `GET /api/folders/{path}?snapshot=date` - Get folder breakdown
- `GET /api/folders/{path}/tree?snapshot=date` - Get folder tree structure
- `GET /api/folders/{path}/timeline?start=date&end=date` - Folder timeline
- `GET /api/folders/{path}/types?snapshot=date` - File type distribution

### Analytics

- `GET /api/analytics/heavy-files?snapshot=date` - Largest files
- `GET /api/analytics/inactive-files?snapshot=date&days=365` - Inactive files
- `GET /api/analytics/recent-activity?snapshot=date` - Recently modified files
- `GET /api/analytics/duplicates?snapshot=date` - Potential duplicate files
- `GET /api/analytics/growth?from=date&to=date` - Storage growth analysis
- `GET /api/analytics/distribution?snapshot=date` - Size/type distribution

## Configuration

Configuration is managed through environment variables. Copy `.env.example` to `.env` and adjust:

```bash
# Database paths
DUCKDB_PATH="data/storage_analytics.duckdb"
SNAPSHOTS_PATH="data/snapshots"

# Redis configuration
REDIS_HOST="localhost"
REDIS_PORT=6379
REDIS_ENABLED=true

# Query limits
DEFAULT_SEARCH_LIMIT=1000
MAX_SEARCH_LIMIT=10000

# Logging
LOG_LEVEL="INFO"
```

## Data Directory Structure

The backend expects parquet snapshots in this structure:

```
data/
├── snapshots/
│   ├── 2024-01-15/
│   │   ├── cil.parquet
│   │   ├── battuta_shares.parquet
│   │   └── gcp.parquet
│   ├── 2024-02-15/
│   │   └── ...
│   └── 2024-03-15/
│       └── ...
└── storage_analytics.duckdb
```

## Development

### Running Tests

```bash
# Run all tests
./scripts/run_tests.sh

# Run specific test file
pytest tests/test_database/test_duckdb_client.py -v

# Run with coverage
pytest --cov=app --cov-report=html
```

### Code Quality

```bash
# Format code
black app/ tests/

# Lint code
ruff check app/ tests/

# Type checking
mypy app/
```

### Generating Test Data

```bash
# Generate mock parquet files for testing
python tests/fixtures/generate_test_data.py
```

## Performance Tuning

### DuckDB Optimization

The DuckDB client is configured for optimal performance:

- Thread pool: 4 threads
- Memory limit: 4GB
- Parquet files read with union_by_name for schema flexibility

### Redis Caching

Cache TTLs are configured based on data volatility:

- Search results: 1 hour
- Folder breakdowns: 2 hours
- Snapshot list: 24 hours

### Query Performance

- Use specific snapshot dates instead of "latest" when possible
- Limit result sets appropriately
- Heavy queries are automatically cached

## Deployment

### Production Deployment

1. **Set environment variables**:
   ```bash
   export DUCKDB_PATH="/path/to/production.duckdb"
   export SNAPSHOTS_PATH="/path/to/snapshots"
   export REDIS_HOST="production-redis"
   export LOG_LEVEL="WARNING"
   ```

2. **Run with production server**:
   ```bash
   gunicorn app.main:app -w 4 -k uvicorn.workers.UvicornWorker --bind 0.0.0.0:8000
   ```

3. **Or use Docker**:
   ```bash
   docker-compose -f docker-compose.prod.yml up -d
   ```

### Monitoring

- Health check endpoint: `/health`
- Metrics: Configure with Prometheus (optional)
- Logs: Structured JSON logging for production

## Troubleshooting

### "No snapshots found"

Ensure parquet files exist in `SNAPSHOTS_PATH` directory with the correct structure.

### Redis connection errors

If Redis is unavailable, caching is automatically disabled. The API will still work but may be slower.

### Query timeout

Increase `QUERY_TIMEOUT` in configuration for complex queries on large datasets.

### Memory issues

Adjust DuckDB memory limit in `duckdb_client.py`:
```python
self.conn.execute("SET memory_limit = '8GB'")
```

## Contributing

1. Create a feature branch
2. Write tests for new functionality
3. Ensure all tests pass: `pytest`
4. Format code: `black app/ tests/`
5. Lint code: `ruff check app/ tests/`
6. Submit pull request

## License

[Specify License]

## Support

For issues and questions:
- GitHub Issues: [Link to issues]
- Documentation: [Link to docs]

---

**Phase 2 Status**: ✅ Complete

Next: Phase 3 - Frontend Implementation
